{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project parses 3 TB nested Json file into csv using pyspark along with sparksql for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention !! All the data used is test data for only partial data from Dec 30,2015. For complete version, please check other files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Json, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from scipy.interpolate import interp1d # import dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bus_file='test.jsons'\n",
    "bus = sqlContext.read.json(bus_file)\n",
    "bus.registerTempTable(\"bus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bus.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and apply SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"spark_extract.sql\") as fr:\n",
    "     query = fr.read()\n",
    "output = sqlContext.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def extract(parts):\n",
    "    for p in parts:\n",
    "        for o in itertools.izip(p.Line,p.Latitude,p.Longitude,p.RecordedAtTime,p.vehicleID,p.Trip,p.TripDate):\n",
    "            yield o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_list(p):\n",
    "    if p.ROUTE_ID!=None:\n",
    "        return zip(p.ROUTE_ID,p.latitude,p.longitude,p.recorded_time\\\n",
    "                   ,p.vehicle_id,p.TRIP_ID,p.tripdate,p.SHAPE_ID\\\n",
    "                   ,p.STOP_ID,p.distance_stop,p.distance_shape,p.status,p.destination)\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranfer time to Unix time for interpolatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import dateutil.parser\n",
    "def unix_time(x):\n",
    "    dt = dateutil.parser.parse(x)\n",
    "    return time.mktime(dt.timetuple())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findIncreasingList(parts):\n",
    "    prev = 0\n",
    "    for record in parts:\n",
    "        if record[-1]<prev:\n",
    "            return\n",
    "        prev = record[-1]\n",
    "        yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d # import dependency\n",
    "def predict(x):\n",
    "    pre_x = [p[-1] for p in x if p[-1]!=None]\n",
    "    if len(pre_x) >= 2:\n",
    "        pre_y = [unix_time(p[1]) for p in x if p[-1]!=None]\n",
    "        f = interp1d(pre_x, pre_y)\n",
    "    else:\n",
    "        return []\n",
    "    return findIncreasingList([(p[0],p[2],p[3],f(p[-1]+p[-2]))\\\n",
    "                               for p in x if p[-1]!=None and (p[-1]+p[-2]) <= pre_x[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "def predict_map(x):\n",
    "    train_y = [unix_time(p[3]) for p in x if p[-3]!=None ]\n",
    "    if len(train_y) >= 2:\n",
    "        train_x = [p[-3] for p in x if p[-3]!=None]\n",
    "        f = interp1d(train_x, train_y)\n",
    "        distance = [(p[-3]+p[-4]) for p in x \\\n",
    "                    if p[-3]!=None and (p[-3]+p[-4]) <= train_x[-1]]\n",
    "        stoptimes = f(distance)\n",
    "        stops = [p[-5] for p in x if p[-3]!=None]\n",
    "    else:\n",
    "        return[]\n",
    "    return map(lambda a,b: (a,b), stops,stoptimes)\n",
    "    #return [(p[-4],f(p[-2]+p[-3])) for p in x if (p[-2]!=None and p[-3]!=None) and (p[-2]+p[-3]) <= pre_x[-1]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby Date and Line & Apply Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group By TRIP_ID and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .map(lambda x:((x[5],x[6]),x)).groupByKey()\\\n",
    "      .map(lambda x: x[1])\\\n",
    "      .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupbykey and Apply Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .map(lambda x:((x[5],x[6]),(x[0],x[3],x[5],x[8],x[-4],x[-3])))\\\n",
    "      .groupByKey()\\\n",
    "      .flatMap(lambda x: predict(x[1]))\\\n",
    "      .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the prefix, timezones and save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .map(lambda x:((x[5],x[6]),(x[0],)).groupByKey() \\\n",
    "      .flatMap(lambda x: predict(x[1]))\\\n",
    "      .map(lambda x: \",\".join(map(str, x)))\\\n",
    "      .map(lambda x: x.replace('MTA NYCT_', '').replace('MTABC_','').replace('MTA_','').replace('-05:00',''))\\\n",
    "      .saveAsTextFile('stoptimes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read From CSV and SQL Manupilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Schemas and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customSchema = StructType([StructField(\"ROUTE_ID\", StringType(), True),\\\n",
    "                           StructField(\"latitude\", DoubleType(), True),\\\n",
    "                           StructField(\"longitude\", DoubleType(), True),\\\n",
    "                           StructField(\"recorded_time\", StringType(), True),\\\n",
    "                           StructField(\"vehicle_id\", StringType(), True),\\\n",
    "                           StructField(\"TRIP_ID\", StringType(), True),\\\n",
    "                           StructField(\"tripdate\", DateType(), True),\\\n",
    "                           StructField(\"SHAPE_ID\", StringType(), True),\\\n",
    "                           StructField(\"STOP_ID\", StringType(), True),\\\n",
    "                           StructField(\"distance_stop\", StringType(), True),\\\n",
    "                           StructField(\"distance_shape\", StringType(), True),\\\n",
    "                           StructField(\"status\", StringType(), True),\\\n",
    "                           StructField(\"destination\", StringType(), True)])             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_times_schema = StructType([StructField(\"trip_id\", StringType(), True),\\\n",
    "                           StructField(\"arrival_time\", StringType(), True),\\\n",
    "                           StructField(\"departure_time\", StringType(), True),\\\n",
    "                           StructField(\"stop_id\", StringType(), True),\\\n",
    "                           StructField(\"stop_sequence\", StringType(), True),\\\n",
    "                           StructField(\"pickup_type\", StringType(), True),\n",
    "                           StructField(\"drop_off_type\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_stoptimes_schema = StructType([StructField(\"ROUTE_ID\", StringType(), True),\\\n",
    "                           StructField(\"TRIP_ID\", StringType(), True),\\\n",
    "                           StructField(\"STOP_ID\", StringType(), True),\\\n",
    "                           StructField(\"time\",IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CSV=>DF tool to read saved csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_stoptimes = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('stops.csv', schema = real_stoptimes_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoptimes = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('stop_times.txt',schema = stop_times_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(trip_id=u'EN_U5-Weekday-008000_B12_1', arrival_time=u'01:20:00', departure_time=u'01:20:00', stop_id=u'901471', stop_sequence=u'1', pickup_type=u'0', drop_off_type=u'0'),\n",
       " Row(trip_id=u'EN_U5-Weekday-008000_B12_1', arrival_time=u'01:21:28', departure_time=u'01:21:28', stop_id=u'301386', stop_sequence=u'2', pickup_type=u'0', drop_off_type=u'0'),\n",
       " Row(trip_id=u'EN_U5-Weekday-008000_B12_1', arrival_time=u'01:22:00', departure_time=u'01:22:00', stop_id=u'301387', stop_sequence=u'3', pickup_type=u'0', drop_off_type=u'0'),\n",
       " Row(trip_id=u'EN_U5-Weekday-008000_B12_1', arrival_time=u'01:22:55', departure_time=u'01:22:55', stop_id=u'301388', stop_sequence=u'4', pickup_type=u'0', drop_off_type=u'0'),\n",
       " Row(trip_id=u'EN_U5-Weekday-008000_B12_1', arrival_time=u'01:23:13', departure_time=u'01:23:13', stop_id=u'301389', stop_sequence=u'5', pickup_type=u'0', drop_off_type=u'0'),\n",
       " Row(trip_id=u'EN_U5-Weekday-008000_B12_1', arrival_time=u'01:23:53', departure_time=u'01:23:53', stop_id=u'301390', stop_sequence=u'6', pickup_type=u'0', drop_off_type=u'0'),\n",
       " Row(trip_id=u'EN_U5-Weekday-008000_B12_1', arrival_time=u'01:24:50', departure_time=u'01:24:50', stop_id=u'307120', stop_sequence=u'7', pickup_type=u'0', drop_off_type=u'0'),\n",
       " Row(trip_id=u'EN_U5-Weekday-008000_B12_1', arrival_time=u'01:26:00', departure_time=u'01:26:00', stop_id=u'301392', stop_sequence=u'8', pickup_type=u'0', drop_off_type=u'0'),\n",
       " Row(trip_id=u'EN_U5-Weekday-008000_B12_1', arrival_time=u'01:26:29', departure_time=u'01:26:29', stop_id=u'308274', stop_sequence=u'9', pickup_type=u'0', drop_off_type=u'0'),\n",
       " Row(trip_id=u'EN_U5-Weekday-008000_B12_1', arrival_time=u'01:28:07', departure_time=u'01:28:07', stop_id=u'306348', stop_sequence=u'10', pickup_type=u'0', drop_off_type=u'0')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoptimes.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_time = real_stoptimes.withColumn('realtime',split(pyspark.sql.functions.from_unixtime(real_stoptimes.time), ' ')[1])\\\n",
    "                         .withColumn('date',split(pyspark.sql.functions.from_unixtime(real_stoptimes.time), ' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(trip_id=u'EN_U5-Weekday-008000_B12_1', arrival_time=u'01:20:00', departure_time=u'01:20:00', stop_id=u'901471', stop_sequence=u'1', pickup_type=u'0', drop_off_type=u'0')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoptimes.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+----------+--------+----------+\n",
      "|ROUTE_ID|             TRIP_ID|STOP_ID|      time|realtime|      date|\n",
      "+--------+--------------------+-------+----------+--------+----------+\n",
      "|     Q46|QV_W5-Weekday-110...| 502327|1451520016|19:00:16|2015-12-30|\n",
      "|     Q46|QV_W5-Weekday-110...| 502331|1451520119|19:01:59|2015-12-30|\n",
      "+--------+--------------------+-------+----------+--------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_time.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_stoptimes = stoptimes.withColumn('route_id', split(stoptimes.trip_id,'_')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------+-------+-------------+-----------+-------------+--------+\n",
      "|             trip_id|arrival_time|departure_time|stop_id|stop_sequence|pickup_type|drop_off_type|route_id|\n",
      "+--------------------+------------+--------------+-------+-------------+-----------+-------------+--------+\n",
      "|EN_U5-Weekday-008...|    01:20:00|      01:20:00| 901471|            1|          0|            0|     B12|\n",
      "+--------------------+------------+--------------+-------+-------------+-----------+-------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_stoptimes.arrival_time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_time.registerTempTable('new_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+----------+--------+----------+\n",
      "|ROUTE_ID|             TRIP_ID|STOP_ID|      time|realtime|      date|\n",
      "+--------+--------------------+-------+----------+--------+----------+\n",
      "|     Q46|QV_W5-Weekday-110...| 502327|1451520016|19:00:16|2015-12-30|\n",
      "+--------+--------------------+-------+----------+--------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_time.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_stoptimes.registerTempTable('stoptimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def get_sec(s):\n",
    "    l = s.split(':')\n",
    "    return int(l[0]) * 3600 + int(l[1]) * 60 + int(l[2])\n",
    "sqlContext.registerFunction(\"getsec\", lambda x: get_sec(x), IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply getsec function to sec and calculate the delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 3), (2, 4), (3, 5)]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5]\n",
    "list(enumerate(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def headway(a):\n",
    "    ini = 0\n",
    "    for index,i in enumerate(a):\n",
    "            if abs(i- a[index-1]) > 180:\n",
    "                ini+=1\n",
    "    return ini\n",
    "sqlContext.registerFunction(\"headway\", lambda x: headway(x), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_sum(a):\n",
    "    for i in enumerate(a):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "join = sqlContext.sql('SELECT * \\\n",
    "                       FROM new_time\\\n",
    "                       INNER JOIN stoptimes\\\n",
    "                       ON TRIP_ID = trip_id AND STOP_ID = stop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join.registerTempTable('new_join')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the performance on 3 ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On Time performance: if the bus arrives 1min ahead of the schedule or 5mins after the schedule. It is ontime\n",
    "2. Peakhour wait assesment:if the bus arrives 3min ahead or 3min after the scheduled time on 6-9 or 16-19. It is ontime\n",
    "3. off-peak hour wait assesement: if the bus arrives within 5mins of the schedule except peak hours. It is ontime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new = sqlContext.sql('SELECT ROUTE_ID,STOP_ID, date,\\\n",
    "                      COUNT(IF((delay BETWEEN -60 AND 300),1,null))/COUNT(delay) as ontime_ratio,\\\n",
    "                      COUNT(IF((HOUR(realtime) BETWEEN 6 AND 9) OR (HOUR(realtime) BETWEEN 16 AND 19) AND (delay BETWEEN -300 AND 300),1,null))/COUNT(IF((HOUR(realtime) BETWEEN 6 AND 9) OR (HOUR(realtime) BETWEEN 16 AND 19),1,null)) as peak_wait,\\\n",
    "                      COUNT(IF((HOUR(realtime) NOT BETWEEN 6 AND 9) OR (HOUR(realtime) NOT BETWEEN 16 AND 19) AND (delay BETWEEN -300 AND 300),1,null))/COUNT(IF((HOUR(realtime) NOT BETWEEN 6 AND 9) OR (HOUR(realtime) NOT BETWEEN 16 AND 19),1,null)) as peak_wait\\\n",
    "                      FROM new_join\\\n",
    "                      GROUP BY ROUTE_ID, STOP_ID, date').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Tansfer to UnixTimeStamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the trips of each line of everyday to test the data intergrety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gaps = sqlContext.sql('SELECT Route_Id, tripdate, count(recorded_time) AS trips\\\n",
    "                       FROM record\\\n",
    "                       GROUP BY Route_Id, tripdate\\\n",
    "                       ORDER BY tripdate DESC') #apply sql Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
